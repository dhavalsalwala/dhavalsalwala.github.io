<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | </title>
    <link>https://dhavalsalwala.github.io/tag/deep-learning/</link>
      <atom:link href="https://dhavalsalwala.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>2021 © Dhaval Salwala</copyright><lastBuildDate>Tue, 10 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dhavalsalwala.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Deep Learning</title>
      <link>https://dhavalsalwala.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Vision-Farm</title>
      <link>https://dhavalsalwala.github.io/project/vision-farm/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://dhavalsalwala.github.io/project/vision-farm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GNOSIS</title>
      <link>https://dhavalsalwala.github.io/project/gnosis/</link>
      <pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://dhavalsalwala.github.io/project/gnosis/</guid>
      <description>&lt;p&gt;GNOSIS is a Multimodal Containerized framework that blends neural (DNNs) and symbolic (rules) models within the event-based paradigm to process IoMT multimodal streams utilising hybrid processing through video event knowledge graphs. GNOSIS combines deep learning and computer vision with real-time event processing capabilities within a microservice-based architecture for video stream processing and AI analytics in Complex Event Processing scenarios. GNOSIS simplifies the querying of streams and the deployment and optimisation of multiple pipelines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Master&#39;s Thesis</title>
      <link>https://dhavalsalwala.github.io/thesis/postgraduate/</link>
      <pubDate>Sat, 31 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://dhavalsalwala.github.io/thesis/postgraduate/</guid>
      <description>&lt;p style=&#39;text-align: justify;&#39;&gt;
This thesis demonstrates the learning course of multi-agent deep reinforcement learning
algorithms in a setup called Pursuit Evasion. Three multi-agent versions of the RL
algorithms have been explored in this research viz. Deep Q networks (DQN), Reinforce
and Advantage Actor-Critic (A2C). The evaluation results reveal that Reinforce has a
better convergence rate in capturing the spatial correlation of the domain using deep
learning. A2C is a slow learner and could do better with parameter refinements. In the
experiments performed in this thesis, DQN did not perform well owing to the non-
stationarity of the domain.
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Agent Deep Reinforcment Learning</title>
      <link>https://dhavalsalwala.github.io/projects_academic/multi-agent_rl/</link>
      <pubDate>Sat, 31 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://dhavalsalwala.github.io/projects_academic/multi-agent_rl/</guid>
      <description>&lt;p style=&#39;text-align: justify;&#39;&gt;
Pursuit is a standard task for benchmarking multi-agent algorithms. The pursuit-evasion domain consists of two sets of agents: evaders and pursuers. The evaders are trying to avoid pursuers, while the pursuers are trying to catch the evaders. The action and observation spaces in this problem are discrete. The agents interact on a two-dimensional grid, and an evader is considered caught if it is surrounded by pursuers on four sides. In order to catch the evaders, the pursuers must learn to cooperate by trapping the evaders on all sides. When the pursuers catch an evader, they receive a reward. The evaders follow a uniform random policy. The domain contains obstacles through which the agents cannot pass. Each pursuer receives a range-limited observation of its surroundings, and must choose between five actions Stay, Go East, Go West, Go South, Go North. The observations contain information about the agent’s surroundings, including the location of nearby pursuers, evaders, and obstacles.
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reinforcement Learning</title>
      <link>https://dhavalsalwala.github.io/projects_academic/rl_algos/</link>
      <pubDate>Sat, 31 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://dhavalsalwala.github.io/projects_academic/rl_algos/</guid>
      <description>&lt;p style=&#39;text-align: justify;&#39;&gt;
All code is written in Python 3 and uses RL environments from OpenAI Gym and MADRL. Advanced techniques use Tensorflow and Keras for neural network implementations.
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
